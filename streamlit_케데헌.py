# ë©”ì¸ í˜ì´ì§€ ì„¤ì •
import streamlit as st
import pandas as pd
# íƒ€ì´í‹€ í…ìŠ¤íŠ¸ ì¶œë ¥
st.title('C117023 ê¹€ë¯¼ì„±')
st.header('KíŒ ë°ëª¬ í—Œí„°ìŠ¤ ì˜¨ë¼ì¸ ë°ì´í„° ë¶„ì„')
st.subheader('íŒ¬ë¤ í˜•ì„± í•µì‹¬ ìš”ì¸ ë‹¤ê°ë„ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ')


st.set_page_config(                        # í˜ì´ì§€ ì„¤ì •
    page_title="3ì°¨ì‹œí—˜_ê¹€ë¯¼ì„±ì˜ Streamlit",        # í˜ì´ì§€ Tabì˜  íƒ€ì´í‹€ 
    page_icon="ğŸ”¥",                        # í˜ì´ì§€ Tabì˜  ì•„ì´ì½˜
    layout="wide",                         # í˜ì´ì§€ ë ˆì´ì•„ì›ƒ: centered, wide
    # ì‚¬ì´ë“œë°” ì´ˆê¸° ìƒíƒœ: auto, collapsed, expanded
    initial_sidebar_state="expanded",
)


st.sidebar.title('ë‹¤ì–‘í•œ ì‚¬ì´ë“œë°” ìœ„ì ¯ë“¤')


########################################################3
# 5ê°œ ì´ìƒ ìœ„ì ¯





# ##############################
# WordCloud ì‹œê°í™”
import re
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from matplotlib import font_manager


@st.cache_data
def load_data():
    return pd.read_csv("kpopdemonhunters.csv")

df = load_data()
st.dataframe(df.head())

# ë¶„ì„ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
text_raw = " ".join(df["title"].astype(str).tolist())

def clean_text(text):
    text = re.sub(r"<[^>]*>", "", text)
    text = re.sub(r"[^\w\sã„±-ã…ã…-ã…£ê°€-í£]", "", text)
    return text

text_cleaned = clean_text(text_raw)


# ì‚¬ì´ë“œë°” ìœ„ì ¯1 ìŠ¬ë¼ì´ë”
st.sidebar.header("ì›Œë“œí´ë¼ìš°ë“œ ì˜µì…˜")
max_words = st.sidebar.slider("ìµœëŒ€ ë‹¨ì–´ ìˆ˜", 10, 100, 50)

background_color = "black"

# ë¶ˆìš©ì–´ ì„¤ì •
stop_str = "kpop ë°ëª¬ í—Œí„°ìŠ¤ ì½˜í…ì¸  ì‘í’ˆ íŒ¬ë¤ ì˜ìƒ ì¥ë©´ ê³µê°œ"
stop_words = set(stop_str.split(" "))
STOPWORDS.update(stop_words)


# í•œê¸€ í°íŠ¸ ì„¤ì •
han_font_path = font_manager.findfont("Gulim")

# WordCloud ìƒì„±
st.subheader("íŒ¬ë¤ í•µì‹¬ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")

wordcloud = WordCloud(
    font_path=han_font_path,
    max_words=max_words,
    stopwords=STOPWORDS,
    background_color=background_color,
    width=800,
    height=800,
    colormap="coolwarm"
).generate(text_cleaned)

fig, ax = plt.subplots(figsize=(7, 7))
ax.imshow(wordcloud)
ax.axis("off")

st.pyplot(fig)

st.divider()
##########################################################################
# ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
st.subheader(" í‚¤ì›Œë“œ ê´€ê³„ ë„¤íŠ¸ì›Œí¬")

import networkx as nx
from konlpy.tag import Okt
from itertools import combinations
from collections import Counter

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
descriptions = df["description"].astype(str).tolist()

okt = Okt()

# 7 ë¶ˆìš©ì–´ ì‚¬ì „ ë¶ˆëŸ¬ì˜¤ê¸°
with open('korean_stopwords.txt', 'r', encoding='utf-8') as f:
    stopwords = f.read().splitlines()

all_nouns = []

for text in descriptions:
    # í•œê¸€ë§Œ ë‚¨ê¸°ê¸°
    text_cleaned = re.sub(r"[^ê°€-í£\s]", "", text)
    # ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text_cleaned)
    # í•œ ê¸€ì ì œê±°
    nouns = [word for word in set(nouns) if len(word) > 1 and (word not in stopwords)]
    all_nouns.append(nouns)

# Edge ë¦¬ìŠ¤íŠ¸ ìƒì„± 
edge_list = []

for nouns in all_nouns:
    if len(nouns) > 1:
        edge_list.extend(combinations(sorted(nouns), 2))

edge_counts = Counter(edge_list)

# ê°€ì¥ ë§ì´ ë“±ì¥í•œ 10ê°œì˜ ì—£ì§€ ì¶œë ¥
print(edge_counts.most_common(10))

min_count = 20
filtered_edges = {edge: weight for edge, weight in edge_counts.items() if weight >= min_count}
st.write(f"ë„¤íŠ¸ì›Œí¬ì— ì‚¬ìš©ëœ ì—£ì§€ ìˆ˜: {len(filtered_edges)}")


# NetworkX ê·¸ë˜í”„ ìƒì„±
G = nx.Graph()

for (node1, node2), weight in filtered_edges.items():
    G.add_edge(node1, node2, weight=weight)

# ì‹œê°í™” (spring layout)
# ë ˆì´ì•„ì›ƒ ìƒì„±
pos_spring = nx.spring_layout(
    G, # ê·¸ë˜í”„ ê°ì²´
    k=0.3, # ë…¸ë“œ ê°„ê²© ì¡°ì ˆ íŒŒë¼ë¯¸í„°
    iterations=50, # ë°˜ë³µ íšŸìˆ˜
    seed=42
)

# 9 ë…¸ë“œ í¬ê¸° ì„¤ì • (ì°¨ìˆ˜ ê¸°ë°˜)
node_sizes = [G.degree(node) * 100 for node in G.nodes()]

# 12 ì—£ì§€ ë‘ê»˜ ì„¤ì • (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
edge_widths = [G[u][v]['weight'] * 0.05 for u, v in G.edges()]

# 15 ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
fig, ax = plt.subplots(figsize=(15, 15))

nx.draw_networkx(
    G,
    pos_spring,
    with_labels=True,
    node_size=node_sizes,
    width=edge_widths,
    font_family=plt.rcParams['font.family'],
    font_size=12,
    node_color='skyblue',
    edge_color='gray',
    alpha=0.8,
    ax=ax
)

ax.set_title("K-POP ë°ëª¬ í—Œí„°ìŠ¤ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬", size=18)
ax.axis("off")

st.pyplot(fig)

st.divider()
######################################################
# Seaborn ê·¸ë˜í”„
# ë§‰ëŒ€ê·¸ë˜í”„
import pandas as pd
import seaborn as sns


df = pd.read_csv("kpopdemonhunters.csv")

df.head()

# ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
descriptions = df["description"].astype(str).tolist()

okt = Okt()

all_nouns = []

for text in descriptions:
    # í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°
    text_cleaned = re.sub(r"[^ê°€-í£\s]", "", text)
    # ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text_cleaned)
    # í•œ ê¸€ì ë‹¨ì–´ ì œê±°
    nouns = [word for word in nouns if len(word) > 1]
    all_nouns.extend(nouns)

word_count = Counter(all_nouns)

top_words = word_count.most_common(10)

# ë°ì´í„°í”„ë ˆì„ ë³€í™˜
# ai ì½”ë“œ ì°¸ì¡°(ê·¸ë˜í”„ ìƒì„±ë¶€ë¶„)
word_df = pd.DataFrame(top_words, columns=["keyword", "count"])

word_df

st.subheader("K-POP ë°ëª¬ í—Œí„°ìŠ¤ í‚¤ì›Œë“œ ë¹ˆë„ (Seaborn)")

fig, ax = plt.subplots(figsize=(8, 5))

# ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
sns.barplot(
    data=word_df,
    x="count",
    y="keyword",
    ax=ax
)

ax.set_title("ë‰´ìŠ¤ ê¸°ì‚¬ ê¸°ë°˜ í‚¤ì›Œë“œ ì–¸ê¸‰ ë¹ˆë„")
ax.set_xlabel("ì–¸ê¸‰ íšŸìˆ˜")
ax.set_ylabel("í‚¤ì›Œë“œ")

st.pyplot(fig)

#####################################################
# Altair
import altair as alt
df = pd.read_csv("kpopdemonhunters.csv")

# ë‚ ì§œ ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
df["pubDate"] = pd.to_datetime(df["pubDate"])

# ë‚ ì§œë§Œ ì¶”ì¶œ
df["date"] = df["pubDate"].dt.date

# ë‚ ì§œë³„ë¡œ ê¸°ì‚¬ ê°œìˆ˜ ì„¸ê¸°
date_count = (
    df.groupby("date")
      .size()
      .reset_index(name="count")
)
date_count.head()

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
# ai ì½”ë“œ ì°¸ì¡°(ê·¸ë˜í”„ ìƒì„±ë¶€ë¶„)
st.subheader("K-POP ë°ëª¬ í—Œí„°ìŠ¤ ë‰´ìŠ¤ ì–¸ê¸‰ ì¶”ì´")
chart = (
    alt.Chart(date_count)
    .mark_line(point=True)
    .encode(
        x=alt.X("date:T", title="ë‚ ì§œ"),
        y=alt.Y("count:Q", title="ê¸°ì‚¬ ìˆ˜"),
        tooltip=["date:T", "count:Q"]
    )
)
st.altair_chart(chart, use_container_width=True)

st.text('ë‰´ìŠ¤ ê¸°ì‚¬ ê°œìˆ˜ê°€ 12ì›” ì´ˆê¹Œì§€ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ì¸ ê²ƒì„ ë³´ì•„ ì˜¤ë«ë™ì•ˆ ì´ìŠˆí™”ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.')
st.divider()
#########################################################3
# Plotly
import plotly.express as px

df = pd.read_csv("kpopdemonhunters.csv")

descriptions = df["description"].astype(str).tolist()

okt = Okt()

nouns_all = []

for text in descriptions:
    text_cleaned = re.sub(r"[^ê°€-í£\s]", "", text)
    nouns = okt.nouns(text_cleaned)
    nouns = [word for word in nouns if len(word) > 1]
    nouns_all.extend(nouns)

word_count = Counter(nouns_all)

# ai ì½”ë“œ ì°¸ì¡°(ê·¸ë˜í”„ ìƒì„±ë¶€ë¶„)
st.subheader("íŒ¬ë¤ í•µì‹¬ í‚¤ì›Œë“œ ë¹ˆë„ (Plotly)")

fig = px.bar(
    word_df,
    x="count",
    y="keyword",
    orientation="h",
    title="K-POP ë°ëª¬ í—Œí„°ìŠ¤ í•µì‹¬ í‚¤ì›Œë“œ ë¹ˆë„{top_n}",
    labels={
        "count": "ì–¸ê¸‰ íšŸìˆ˜",
        "keyword": "í‚¤ì›Œë“œ"
    }
)

st.plotly_chart(fig, use_container_width=True)

# ai ì½”ë“œ ê·¸ëŒ€ë¡œ ì°¸ê³ 
# ìŠ¬ë¼ì´ë” ìœ„ì ¯ (ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜)
top_n = st.slider(
    "ë³´ê³  ì‹¶ì€ í‚¤ì›Œë“œ ê°œìˆ˜ ì„ íƒ",
    min_value=5,
    max_value=30,
    value=10,
    step=1
)

top_words = word_count.most_common(top_n)


word_df = pd.DataFrame(top_words, columns=["keyword", "count"])
